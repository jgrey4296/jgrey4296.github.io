* Natural Language Processing:
** Links:
   http://www.kellogg.northwestern.edu/research/fordcenter/events/text%20as%20data%20oct%202014.htm
   #classification: http://www.nltk.org/howto/classify.html
   #chunk: http://www.nltk.org/howto/chunk.html
   #noun phrase chunking

** nltk:
   I{corpus}.words(): list of str
   I{corpus}.sents(): list of (list of str)
   I{corpus}.paras(): list of (list of (list of str))
   I{corpus}.tagged_words(): list of (str,str) tuple
   I{corpus}.tagged_sents(): list of (list of (str,str))
   I{corpus}.tagged_paras(): list of (list of (list of (str,str)))
   I{corpus}.chunked_sents(): list of (Tree w/ (str,str) leaves)
   I{corpus}.parsed_sents(): list of (Tree with str leaves)
   I{corpus}.parsed_paras(): list of (list of (Tree with str leaves))
   I{corpus}.xml(): A single xml ElementTree
   I{corpus}.raw(): str (unprocessed corpus contents)

*** links:

    bigrams, trigrams, collocations: http://www.nltk.org/howto/collocations.html
    discourse checking: http://www.nltk.org/howto/discourse.html
    framenet: http://www.nltk.org/howto/framenet.html
    generation of sentences: http://www.nltk.org/howto/generate.html
    metrics:  http://www.nltk.org/howto/metrics.html
    parsing: http://www.nltk.org/howto/parse.html
    probability: http://www.nltk.org/howto/probability.html
    entity extraction: http://www.nltk.org/howto/relextract.html
    stemming: http://www.nltk.org/howto/stem.html
    word sense disambiguation: http://www.nltk.org/howto/wsd.html

    penn treebank: http://www.nltk.org/howto/propbank.html
    sentiWordnet: http://www.nltk.org/howto/sentiwordnet.html
    wordnet: http://www.nltk.org/howto/wordnet.html

    proving? http://www.nltk.org/howto/inference.html
    logic: http://www.nltk.org/howto/logic.html
    resolution? http://www.nltk.org/howto/resolution.html

*** Parts of speech:

    http://www.chompchomp.com/terms.htm
    verb: action or state.
    noun: thing or person.
    adjective: describes noun. two dogs. the dogs are big.
    adverb: describes a verb: dog eats quickly.
    pronoun: replaces noun. he she it they etc.
    preposition: links noun with something
    conjunction: joins clauses
    interjection: 'well, I don`t know.'

*** Lexical Categories
    adjective (A)
    adposition (preposition
    postposition
    circumposition) (P)
    adverb (Adv)
    coordinate conjunction (C)
    determiner (D)
    interjection (I)
    noun (N)
    particle (Par)
    pronoun (Pr)
    subordinate conjunction (Sub)
    verb (V)

*** Phrasal categories
    Adjective phrase (AP)
    adverb phrase (AdvP)
    adposition phrase (PP)
    noun phrase (NP)
    verb phrase (VP)

*** Penn Treebank:
    CC Coordinating conjunction

    For:  rationale ("They do not gamble or smoke, for they are ascetics.")
    And:  non-contrasting item(s) or idea(s) ("They gamble and they smoke.")
    Nor:   a non-contrasting negative idea ("They do not gamble, nor do....")
    But:  a contrast or exception ("They gamble, but they don't smoke.")
    Or:   an alternative item or idea ("Every day they gamble or they smoke.")
    Yet:  a contrast or exception ("They gamble, yet they don't smoke.")
    So:   a consequence ("He gambled well last night so he smoked a cigar ...")

    Contrasting Conjunctions:
    either...or
    not only...but (also)
    neither...nor
    both...and
    whether...or
    just as...so

    Subordinating Conjunctions:
    Subordinating conjunctions, also called subordinators, are conjunctions that join an independent clause
    and a dependent clause, and also introduce adverb clauses. The most
    common subordinating conjunctions in the English language include
    after, although, as, as far as, as if, as long as, as soon as, as
    though, because, before, "even if", "even though", "every time" if, in
    order that, since, so, so that, than, though, unless, until, when,
    whenever, where, whereas, wherever, and while.



    CD Cardinal number ("zero", "one")
    DT Determiner
    in some languages articles, in some affixes
    - articles
      - definite (specifically. "The")
      - indefinite (non-specific, "a", "some")
    - demonstratives
    - possessives (my, your, his, whose, its, our)
      - genitives (saxon, "one's", "everybody's", "Mary's")
    - quantifiers
      - basic quantifiers
      - comparative ("more", "less")
      - superlative ("most", "least", "biggest")
      - first order logic ("all", "some", "enough", "sufficient")
    - numerals
    - ennumeratives ("each", "every")
    - personal ("you", "we", "us")
    - distributives
    - interrogatives ("which", "what" etc)
    EX Existential there
    FW Foreign word
    IN Preposition or subordinating conjunction
    JJ Adjective
    JJR Adjective, comparative
    JJS Adjective, superlative
    LS List item marker
    MD Modal
    NN Noun, singular or mass
    NNS Noun, plural
    NNP Proper noun, singular
    NNPS Proper noun, plural
    PDT Predeterminer
    POS Possessive ending
    PRP Personal pronoun
    PRP$ Possessive pronoun
    RB Adverb
    RBR Adverb, comparative
    RBS Adverb, superlative
    RP Particle
    SYM Symbol
    TO to
    UH Interjection
    VB Verb, base form
    VBD Verb, past tense
    VBG Verb, gerund or present participle
    VBN Verb, past participle
    VBP Verb, non-3rd person singular present
    VBZ Verb, 3rd person singular present
    WDT Wh-determiner
    WP Wh-pronoun
    WP$ Possessive wh-pronoun
    WRB Wh-adverb

*** Open and Closed classes:
    Open:
    nouns, vers, adjectives, adverbs, interjections

    Closed:
    adpositions, determiners, conjunctions, pronouns

*** Named Entities:
    NE Type       Examples
    ORGANIZATION      Georgia-Pacific Corp., WHO
    PERSON        Eddy Bonte, President Obama
    LOCATION      Murray River, Mount Everest
    DATE        June, 2008-06-29
    TIME        two fifty a m, 1:30 p.m.
    MONEY         175 million Canadian Dollars, GBP 10.40
    PERCENT       twenty pct, 18.75 %
    FACILITY      Washington Monument, Stonehenge
    GPE         South East Asia, Midlothian

** DataSets:
   Conceptnet
   verbnet
   wordnet
   CDC Datasets
** Analysis:
*** General template
**** For a single Text:
     Read a file in
     Process
     Append Data to <textName>.data

**** For a group of Text:
     Read all <textName>.data files
     process
     output to <groupName>.data

*** Vocabs
**** For each text:
     List of words

**** Then for all texts together:
     Unique words in a text
     Shared words between texts
     Categorised by POS tag.

*** Metrics

    Stats on unique vs shared words.

*** Collocations

    Bigrams, Trigrams for each text.
    Into Generative grammars?

*** Parsing
*** Word/Verb/Concept net exploration:
    plug in a word, get linked words
*** Speech Extraction
    with parsed, extract sentences indicating speech
    (using net exploration)
*** Entity Extraction
    NER, and also noun phrase extraction
    possible comparison to wikipedia
*** Entity - Verb combinations
*** Classification
**** General Test Classification stuff
     classification: http://www.nltk.org/howto/classify.html


     Classifiers label tokens with category labels (or class
     labels). Typically, labels are represented with strings (such as
     "health" or "sports". In NLTK, classifiers are defined using classes
     that implement the ClassifyI interface:

     >>> import nltk
     >>> nltk.usage(nltk.classify.ClassifierI)

     ClassifierI supports the following operations:
     - self.classify(featureset)
     - self.classify_many(featuresets)
     - self.labels()
     - self.prob_classify(featureset)
     - self.prob_classify_many(featuresets)

     NLTK defines several classifier classes:

     ConditionalExponentialClassifier
     DecisionTreeClassifier
     MaxentClassifier
     NaiveBayesClassifier
     WekaClassifier

     Classifiers are typically created by training them on a training corpus.

     1   Regression Tests

     We define a very simple training corpus with 3 binary features: ['a', 'b', 'c'], and are two labels: ['x', 'y']. We use a simple feature set so that the correct answers can be calculated analytically (although we haven't done this yet for all tests).


     >>> train = [
     ...     (dict(a=1,b=1,c=1), 'y'),
     ...     (dict(a=1,b=1,c=1), 'x'),
     ...     (dict(a=1,b=1,c=0), 'y'),
     ...     (dict(a=0,b=1,c=1), 'x'),
     ...     (dict(a=0,b=1,c=1), 'y'),
     ...     (dict(a=0,b=0,c=1), 'y'),
     ...     (dict(a=0,b=1,c=0), 'x'),
     ...     (dict(a=0,b=0,c=0), 'x'),
     ...     (dict(a=0,b=1,c=1), 'y'),
     ...     ]
     >>> test = [
     ...     (dict(a=1,b=0,c=1)), # unseen
     ...     (dict(a=1,b=0,c=0)), # unseen
     ...     (dict(a=0,b=1,c=1)), # seen 3 times, labels=y,y,x
     ...     (dict(a=0,b=1,c=0)), # seen 1 time, label=x
     ...     ]

     Test the Naive Bayes classifier:


     >>> classifier = nltk.classify.NaiveBayesClassifier.train(train)
     >>> sorted(classifier.labels())
     ['x', 'y']
     >>> classifier.classify_many(test)
     ['y', 'x', 'y', 'x']
     >>> for pdist in classifier.prob_classify_many(test):
     ...     print('%.4f %.4f' % (pdist.prob('x'), pdist.prob('y')))
     0.3203 0.6797
     0.5857 0.4143
     0.3792 0.6208
     0.6470 0.3530
     >>> classifier.show_most_informative_features()
     Most Informative Features
     c = 0                   x : y      =      2.0 : 1.0
     c = 1                   y : x      =      1.5 : 1.0
     a = 1                   y : x      =      1.4 : 1.0
     b = 0                   x : y      =      1.2 : 1.0
     a = 0                   x : y      =      1.2 : 1.0
     b = 1                   y : x      =      1.1 : 1.0

     Test the Decision Tree classifier:


     >>> classifier = nltk.classify.DecisionTreeClassifier.train(
     ...     train, entropy_cutoff=0,
     ...                                                support_cutoff=0)
     >>> sorted(classifier.labels())
     ['x', 'y']
     >>> print(classifier)
     c=0? .................................................. x
     a=0? ................................................ x
     a=1? ................................................ y
     c=1? .................................................. y

     >>> classifier.classify_many(test)
     ['y', 'y', 'y', 'x']
     >>> for pdist in classifier.prob_classify_many(test):
     ...     print('%.4f %.4f' % (pdist.prob('x'), pdist.prob('y')))
     Traceback (most recent call last):
     . . .
     NotImplementedError

     Test SklearnClassifier, which requires the scikit-learn package.


     >>> from nltk.classify import SklearnClassifier
     >>> from sklearn.naive_bayes import BernoulliNB
     >>> from sklearn.svm import SVC
     >>> train_data = [({"a": 4, "b": 1, "c": 0}, "ham"),
     ...               ({"a": 5, "b": 2, "c": 1}, "ham"),
     ...               ({"a": 0, "b": 3, "c": 4}, "spam"),
     ...               ({"a": 5, "b": 1, "c": 1}, "ham"),
     ...               ({"a": 1, "b": 4, "c": 3}, "spam")]
     >>> classif = SklearnClassifier(BernoulliNB()).train(train_data)
     >>> test_data = [{"a": 3, "b": 2, "c": 1},
     ...              {"a": 0, "b": 3, "c": 7}]
     >>> classif.classify_many(test_data)
     ['ham', 'spam']
     >>> classif = SklearnClassifier(SVC(), sparse=False).train(train_data)
     >>> classif.classify_many(test_data)
     ['ham', 'spam']

     Test the Maximum Entropy classifier training algorithms; they should all generate the same results.


     >>> def print_maxent_test_header():
     ...     print(' '*11+''.join(['      test[%s]  ' % i
     ...                           for i in range(len(test))]))
     ...     print(' '*11+'     p(x)  p(y)'*len(test))
     ...     print('-'*(11+15*len(test)))



     >>> def test_maxent(algorithm):
     ...     print('%11s' % algorithm, end=' ')
     ...     try:
     ...         classifier = nltk.classify.MaxentClassifier.train(
     ...                         train, algorithm, trace=0, max_iter=1000)
     ...     except Exception as e:
     ...         print('Error: %r' % e)
     ...         return
     ...
     ...     for featureset in test:
     ...         pdist = classifier.prob_classify(featureset)
     ...         print('%8.2f%6.2f' % (pdist.prob('x'), pdist.prob('y')), end=' ')
     ...     print()



     >>> print_maxent_test_header(); test_maxent('GIS'); test_maxent('IIS')
     test[0]        test[1]        test[2]        test[3]
     p(x)  p(y)     p(x)  p(y)     p(x)  p(y)     p(x)  p(y)
     -----------------------------------------------------------------------
     GIS     0.16  0.84     0.46  0.54     0.41  0.59     0.76  0.24
     IIS     0.16  0.84     0.46  0.54     0.41  0.59     0.76  0.24



     >>> test_maxent('MEGAM'); test_maxent('TADM')
     MEGAM   0.16  0.84     0.46  0.54     0.41  0.59     0.76  0.24
     TADM    0.16  0.84     0.46  0.54     0.41  0.59     0.76  0.24

     2   Regression tests for TypedMaxentFeatureEncoding


     >>> from nltk.classify import maxent
     >>> train = [
     ...     ({'a': 1, 'b': 1, 'c': 1}, 'y'),
     ...     ({'a': 5, 'b': 5, 'c': 5}, 'x'),
     ...     ({'a': 0.9, 'b': 0.9, 'c': 0.9}, 'y'),
     ...     ({'a': 5.5, 'b': 5.4, 'c': 5.3}, 'x'),
     ...     ({'a': 0.8, 'b': 1.2, 'c': 1}, 'y'),
     ...     ({'a': 5.1, 'b': 4.9, 'c': 5.2}, 'x')
     ... ]



     >>> test = [
     ...     {'a': 1, 'b': 0.8, 'c': 1.2},
     ...     {'a': 5.2, 'b': 5.1, 'c': 5}
     ... ]



     >>> encoding = maxent.TypedMaxentFeatureEncoding.train(
     ...     train, count_cutoff=3, alwayson_features=True)



     >>> classifier = maxent.MaxentClassifier.train(
     ...     train, bernoulli=False, encoding=encoding, trace=0)



     >>> classifier.classify_many(test)
     ['y', 'x']

*** Scene Extraction
