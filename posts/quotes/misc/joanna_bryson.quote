## joanna_bryson.quote -*- mode: fundamental-mode -*-
# Summary:
#
#
#

While I have no problem with the use of artificial intelligence to compliment and improve human decision making,
suggesting that the AI itself *makes* the decision is a problem.
Legal and moral responsibility for a robot's actions should be no different than they are for any other AI System,
and these are the same as for any other tool.
Ordinarily, damage caused by a tool is the fault of an operator, and benefit from it is to the operator's credit.
If the system malfunctions due to poor manufacturing, then the fault may lay with the company that built it, and
the operator can sue to resolve this.

In contrast, creating a legal or even public-relations framework in which a robot can be blamed is like blaming the privates at Abu Grhaib for
being "bad apples". Yes, some apples are worse than others and perhaps culpably so, but ultimate responsibility lies within the command chain that
created the environment those privates operated in. Where the subject is machines we build and own, then the responsible role of the
organisation is even clearer.
We should never be talking about machines taking ethical decisions, but rather machines operated correctly within the limits we set for them.

- Robots should be slaves
